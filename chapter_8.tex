\section{Conditioning and Disintegration}
\subsection{Conditional Probability}
\subsubsection{The Discrete Case}
Let $(\Omega, \F, \p)$ be a probability space. We recall the notions of conditional probability on an event: 

\begin{definition}[Conditional Probabilities]
Let $B \in \F$ be an event with $\p(B) > 0$. The \textit{conditional probability of $A$ with respect to $B$} is 
\begin{equation}
    \p(A \,|\, B) = \frac{\p(A \cap B)}{\p(B)}, \quad A\in \F.
\end{equation}
By convention, if $\p(B) = 0$ then we set $\p(A \,|\, B) = 0$
\end{definition}

\begin{exercise}
Check that $\p(\cdot | B)$ is a measure on $\F$.
\end{exercise}

Conditioning should be relative to information one has and $\sigma$-algebra are the natural carriers or descriptions for information content. We would thus like to condition on a $\sigma$-algebra. In the example above, we could replace $B$ by its complement $B^c$ and obtain a new measure $\p(A|B^c)$. Now, for any $\omega \in \Omega$, we have either $\omega \in B$ or $\omega \in B^c$ so it is natural to define
\begin{equation}
    \p(A | \sigma(B))(\omega) := \p(A|B)\chi_B(\omega) + \p(A | B^c)\chi_{B^c}(\omega).
\end{equation}
In this way, for a fixed $\omega \in \Omega$, $\p(\cdot | \sigma(B))(\omega)$ is a probability measure but for a fixed $A\in \F$, $\p(A|\sigma(B))(\cdot)$ is a random variable (taking two values).\\

The sets $B$ and $B^c$ partition $\Omega$, and these ideas carry over to the general partition. 
\begin{definition}[Conditional Expectation on $\sigma$-algebra generated by partition]
    Let $D = \{D_1, D_2, \dots \}$ be a finite or countable partition of $\Omega$ and let $\G = \sigma(D)$. For $A \in \F$ consider the function with values
    \begin{equation} \label{eq:conditional_probability_partition}
        f(\omega) := \p(A | D_i) = \frac{\p(A\cup D_i)}{\p(D_i)}, \quad \text{if }\omega \in D_i.
    \end{equation}
    This function or random variable $f$ is called the \textbf{conditional probability of $A$ given $\G$} and is denoted $\p(A | \G)$. This is written as $\p(A | \G)_\omega$ or $[\p(A | \G)](\omega)$, whenever the argument $\omega$ needs to be explicitly shown.
\end{definition}

\begin{exercise}
Convince yourself that if $\G = \set{\emptyset, \Omega}$ then $f(\omega) := [\p(A\,|\,\G)](\omega)$ is the constant function $f(\omega) := \p(A)$.
\end{exercise}

If the observer learns which element $D_i$ of the partition it is that contains $\omega$, then his new probability for the event $\omega \in A$ is $f(\omega)$. The partition $\{D_i\}$, or equivalently the $\sigma$-algebra, $\G$, can be regarded as an experiment, and to learn which $D_i$ it is that contains $\omega$ is to learn the outcome of the experiment. Thus $\p(A | \G)$ is a function whose value on $D_i$ is the ordinary conditional probability $\p(A | D_i)$. This definition needs to be completed, because $\p(A | D_i)$ is not defined if $\p(D_i) = 0$. In this case $\p(A | \G)$ will be taken to have any constant value on $D_i$; the value is arbitrary but must be the same over all of the set $D_i$.
\begin{example}
Suppose that $X_0, X_1, \dots$ is a Markov chain with state space $S$. The events
\begin{equation*}
    \{ X_0 = i_0,\dots, X_n = i_n \}, \quad i_0,\dots, i_n \in S
\end{equation*}
form a finite or countable partition of $\Omega$. If $\G_n$ is the $\sigma$-algebra generated by this partition, then by the defining condition for Markov chains, 
\begin{equation*}
    \p(X_{n+1} = j | \G_n)_\omega = \p(X_{n+1} = j|X_n = i_n),
\end{equation*}
for $\omega \in \{ X_0 = i_0,\dots, X_n = i_n \}$, $i_0,\dots, i_n \in S$. The sets $\{X_n = i \}$ for $i\in S$ also partition $\Omega$, and they generate a $\sigma$-algebra $\G_n^0$ smaller than $\G_n$. Now 
\begin{equation*}
    \p(X_{n+1} = j | \G_n^0)_\omega = \p(X_{n+1} | X_n = i),
\end{equation*}
for $\omega \in \{X_n = i\}, i \in S$, and the essence of the Markov property is that 
\begin{equation*}
    \p(X_{n+1} | \G_n) = \p(X_{n+1}|\G_n^0).
\end{equation*}
\end{example} 
\subsubsection{The General Case}
If $\G$ is the $\sigma$-algebra generated by a partition $D_1, D_2, \dots$, then the general element of $\G$ is a disjoint union $D_{i_1} \cup \D_{i_2} \cup \cdots$, finite or countable, of certain of the $D_i$. To know which set $D_i$ it is that contains $\omega$ is the same thing as to know which sets in $\G$ contain $\omega$ and which do not. This second way of looking at the matter carries over to the general $\sigma-$algebra $\G$ contained in $\F$ (as always, we have the probability space $(\Omega, \F, \p)$). The $\sigma-$algebra $\G$ will not in general come from a partition as above.\\

One can imagine an observer who knows for each $G$ in $\G$ whether $\omega \in G$ or $\omega \in G^c$. Thus the $\sigma$-algebra $\G$ can in principle be identified with an experiment or observation. Is is natural to try and define conditional probabilities $\p(A | \G)$ with respect to the experiment $\G$. To do this, fix an $\A$ in $\F$ and define a finite measure $\nu$ on $\G$ by
\begin{equation}
    \nu (G) = \p(A \cap G), \quad G \in \G.
\end{equation}
Then $\p(G) = 0$ implies that $\nu (G) = 0$. The Radon-Nikodym theorem can be applied to the measures $\nu$ and $\p$ on the measurable space $(\Omega, \G)$ because the first one is absolutely continuous with respect to the second. It follows that there exists a function or a random variable $f$, which is $\G-$measurable and integrable with respect to $\p$, such that 
\begin{equation}
    \p(A \cap G) = \nu (G) = \int_G f \d \p,
\end{equation}
for all $G$ in $\G$. Denote this function $f$ by $\p(A | \G)$. It is a random variable with two properties:
\begin{enumerate}[(i)]
    \item $\p(A | \G)$ is $\G$-measurable and integrable
    \item $\p(A | \G)$ satisfies the functional equation
    \begin{equation} \label{eq:conditional_probability_sigma_algebra}
        \int_G \p(A | \G) \d \p = \p(A \cap G), \quad G \in \G.
    \end{equation}
\end{enumerate}
There will in general be many such random variables $\p(A | \G)$, but any two of them are equal with probability $1$.\\

If $\G$ is generated by a partition $D_1, D_2, \dots$, the function $f$ defined by $(8.1)$ is $\G$-measurable because $\{ \omega: f(\omega) \in H \}$ is the union of those $D_i$ over which the constant value of $f$ lies in $H$. Any $G$ in $\G$ is a disjoint union $G = \bigcup_{k} D_{i_k}$, and 
\begin{equation}
    \p(A \cap G) = \sum_k \p(A | D_{i_k})\p(D_{i_k}),
\end{equation}
so that \eqref{eq:conditional_probability_partition} satisfies \eqref{eq:conditional_probability_sigma_algebra} as well. Thus the general definition is an extension of the one for the discrete case.\\

Condition $(i)$ above requires that the values of $\p(A|\G)$ depend only on the sets in $\G$. An observer who knows the outcome of $\G$ viewed as an experiment knows for each $G$ in $\G$ whether it contains $\omega$ or not. For each $x$ he knows this in particular for the set $\{\omega' : \p(A|\G)_{\omega'} = x\}$, and hence he knows in principle the functional value $\p(A|\G)_\omega$ even if he does not know $\omega$ itself.\\

Condition $(ii)$ has a gambling interpretation. Suppose that the observer, after he has learned the outcome of $\G$ is offered the opportunity to bet on the event $A$ (unless $A$ lies in $\G$, he does not yet know whether or not it occurred). He is required to pay an entry fee of $\p(A|\G)$ units and will win $1$ unit if $A$ occurs and nothing otherwise. If the observer decides to bet and pays the fee, he gains $1 - \p(A|\G)$ if $A$ occurs and $-\p(A | \G$ otherwise, so that his gain is 
\begin{equation*}
    (1 - \p(A | \G)) \chi_A + (-\p(A|\G))\chi_{A^c} = \chi_A - \p(A | \G).
\end{equation*}
If he declines to bet, his gain is of course $0$. Suppose that he adopts the strategy of betting if $G$ occurs but not otherwise, where $G$ is some set in $\G$. He can actually carry out this strategy, since after learning the outcome of the experiment $\G$ he knows whether or not $G$ occurred. His expected gain with this strategy is his gin integrated over $G$:
\begin{equation*}
    \int_G (\chi_A - \p(A | \G)) \d \p.
\end{equation*}
But \eqref{eq:conditional_probability_sigma_algebra} is exactly the requirement that this vanish for each $G$ in $\G$. Condition $(ii)$ requires then that each strategy be fair in the sense that the observer stands neither to win nor to lose on the average. Thus $\p(A | \G)$ is just entry fee, as intuition requires.\\


\subsection{Conditional Expectation}
We now develop the theory of condition expectation from first principles. 

\begin{definition}[Conditional expectation] \label{def:conditional_expectation}
Suppose that $\xi$ is an integrable random variable on $(\Omega, \F, \p)$ and that $\G$ is a $\sigma-$algebra contained in $\F$. There exists a random variable $\E[\xi | \G]$, called \textbf{conditional expectation of $\xi$ given $\G$}, having these two properties:
\begin{itemize}
    \item $\E[\xi | \G]$ is $\G$-measurable and integrable.
    \item For every $G \in \G$,
    \begin{equation} \label{eq:condtional_expectation}
        \E[\chi_G \E[\xi | \G]] = \int_G \E[\xi | \G] \d \p = \int_G \xi \d \p = \E[\chi_G \xi]
    \end{equation}
\end{itemize}
\end{definition}

\begin{theorem}[Existence of conditional expectation] Whenever $\xi$ is an integrabl random variable on $(\Omega, \F, \p)$, and that $\G$ is a $\sigma$-algebra contained in $\F$, then $\E[\xi \,|\, \G]$ exists, and is unique almost everywhere.
\end{theorem}

To prove the existence of such a random variable, we recall the following generalised version of the Randon-Nikodym theorem.

\begin{theorem}[Radon-Nikodym Theorem]
Let $(\Omega, \F)$ be a measure space, $\mu$ - a finite measure on $\F$. Let $\lambda$ be a measure on $\F$ a.c. with respect to $\mu$ (i.e. $\lambda (A) = 0$ whenever $\mu(A)=0)$. Then there exists an $\F$-measurable function $f$ such that 
\begin{equation}
    \lambda(A) = \int_A f \d \mu \quad \forall A \in \F.
\end{equation}
This function is determined uniquely, up to sets of measure zero. It is called the derivative of $\lambda$ w.r.t. $\mu: f = \frac{\d \lambda}{\d \mu}$. 
\end{theorem}

Consider first the case of nonnegative $\xi$. Define a measure $\nu$ on $\G$ by
\begin{equation*}
    \nu(G) = \int_G \xi \, \d \p.
\end{equation*}
This measure is finite because $\xi$ is integrable, and it is absolutely continuous with respect to $\p$. By the Radon-Nikodym theorem there is a function $f$, $\G-$measurable, such that $\nu(G) = \int_G f \d \p$. This $f$ has the two properties of our definition. If $\xi$ is not necessarily nonnegative, $\E[\xi^+ | \G] - \R[\xi^- | \G]$ clearly has the required properties. \\

There will in general be many such random variables $\E[\xi | \G]$. Any one of them is called a version of the conditional expected value. Any two versions are equal with probability $1$.

\begin{example}[Conditioning on $\sigma$-algbera generated by partition]
Suppose that $G_1, G_2, \dots$ is a finite or countable partition of $\Omega$ generating the $\sigma-$algebra $\G$. Then $\E[\xi | \G]$ must, since it is $\G-$measurable, have some constant value over $G_i$, say $\alpha_i$. Then
\begin{align}
    \int_{G_i} \E[\xi | \G] \d \p &= \int_{G_i} \alpha_i \, \d \p = \alpha_i \p(G_i).
\end{align}
Therefore,
\begin{equation}
    \alpha_i \p (G_i) = \int_{G_i} \xi \d \p,
\end{equation}
which implies
\begin{equation}
    \E[\xi | \G]_\omega = \frac{1}{\p(G_i)} \int_{G_i} \xi \d \p, \quad \omega \in \G_i.
\end{equation}
If $\p(G_i) = 0$, then the value of $\E[\xi | \G]$ over $G_i$ is constant but arbitrary.
\end{example}

To familiarise yourself with the definition of conditional expectation on a $\sigma$-algebra, we provide the following exercise.
\begin{exercise} \label{ex:tower_property}
\begin{enumerate}
    \item[]
    \item Convince yourself that if $\G = \set{\emptyset, \Omega}$ then $[\E[\xi\,|\,\G]](\omega)$ is equal to the constant function $f(\omega) := \E(\xi)$ almost everywhere.
    \item More challenging: let's say you know that $\xi$ is independent of $\G$, which means that for all $B \in \G$ we have $\xi$ independent of $\chi_B$, then we still have $\E[\xi\,|\,\G] = \E[\xi]$
    \item Convince yourself that $\E[\xi \,|\, \F] = \xi$ almost everywhere.
    \item Show that $\E[\E[\xi \,|\, \G]] = \E[\xi]$.
\end{enumerate}
\end{exercise}

\begin{hint}
All problems can be solved solely by checking the definition. For example, in question 1 you check that equality \eqref{eq:conditional_probability_sigma_algebra} holds for $G = \phi$ and $G = \Omega$, and in question 4 you directly use the equality \eqref{eq:conditional_probability_sigma_algebra} for appropriate choice of $G \in \G$. You should also try to convince yourself that the above results are intuitive, e.g. in question 3 the conditional expectation is the random variable itself since you know everything about the random variable.
\end{hint}

\begin{solution} (For question 2):
This is perhaps the most tricky question, but again we try to resort to the definition \eqref{eq:conditional_probability_sigma_algebra}. Here we want to show for all $G \in \G$
\begin{equation}
    E[\xi] \E[\chi_G] \E[\chi_G \E[\xi]] = \E[\chi_G \xi]
\end{equation}
which is true by the independence assumption. This makes sense, because the $\sigma$ algebra does not give additional information about the random variable $\xi$.
\end{solution}

The example below links conditional expectation and conditional probabilities.

\begin{example}[Conditional expectation of indicator function]
For an indicator random variable $\chi_A$ the defining properties of $\E[\chi_A | \G]$ and $\p(A| \G)$ coincide, therefore 
\begin{equation}
    \E[\chi_A | \G] = \p(A | \G)
\end{equation}
almost surely. For a simple random variable $\xi = \sum \alpha_i \chi_{A_i}$,
\begin{equation}
    \E[\xi | \G] = \sum \alpha_i \p(A_i | \G)
\end{equation}
almost surely.
\end{example}

\subsection{Properties of conditional expectation}
We list some of the properties for conditional expectation. Most properties are direct applications of the definition \eqref{eq:conditional_probability_sigma_algebra}, and we encourage you to prove them by yourselves. The proofs are, however, still included if you want extra hint to familiarise yourself with the notion of conditional expectation.

\begin{property}[I. Linearity] 
Let $\xi,\eta$ is a random variable on $(\Omega, \F, \p)$ and let $\G$ be a $\sigma-$algebra contained in $\F$. Then, almost surely,
\begin{equation} \label{eq:linearity_conditional_expectation}
    \E[a\xi + b\eta + c \,|\, \G] = a\E[\xi \,|\, \G] + b \E[\eta \,|\, \G ] + c
\end{equation}
\end{property}

\begin{proof}
For all $G \in \G$, we have
\begin{align*}
    \E[\chi_G \E[a\xi + b\eta + c \,|\, \G]] 
    &= \E[\chi_G (a\xi + b\eta + c)] \\
    &= a\E[\chi_G\xi] + b\E[\chi_G \eta] + c\E[\chi_G] \\
    &= a\E[\chi_G \E[\xi\,|\,\G]] + b\E[\chi_G \E[\eta\,|\,\G]] + c\E[\chi_G] \\
    &= \E[\chi_G (a\E[\xi \,|\, \G] + b \E[\eta \,|\, \G ] + c)]
\end{align*}
\end{proof}

\begin{property}[II. Monotonicity] \label{prop:conditional_expectation_monotonicity} 
Let $\xi, \eta$ is a random variable on $(\Omega, \F, \p)$ such that $\xi \leq \eta$ almost surely, and let $\G$ be a $\sigma-$algebra contained in $\F$. Then, almost surely, $\E[\xi\,|\,\G] \leq \E[\eta \,|\, \G]$. The above result also holds when $\geq$ is replaced by $>$.
\end{property}

\begin{proof}
Consider the set $G = \set{\omega\,|\,\E[\xi\,|\,\G] > \E[\eta\,|\,\G]}$. If $\p(G) > 0$ then $\E[\chi_G (\E[\xi\,|\,\G] - \E[\eta\,|\,\G])] > 0$ by our definition of $G$. But we also know that $\E[\chi_G (\E[\xi\,|\,\G] - \E[\eta\,|\,\G])] = \E[\chi_G (\xi-\eta)] \leq 0$, which is a contradiction. So $\p(G) = 0$, and $\E[\chi_G (\E[\xi\,|\,\G] - \E[\eta\,|\,\G])] = 0$. So $\E[(\E[\xi\,|\,\G] - \E[\eta\,|\,\G])] = \E[\chi_{G^C} (\E[\xi\,|\,\G] - \E[\eta\,|\,\G])] \leq 0$, and hence $\E[\xi\,|\,\G] \leq \E[\eta\,|\,\G]$ almost surely.
\end{proof}

Substituting $\xi = 0$, then we know that $\eta \geq 0$ a.s. $\implies$ $\E[\eta\,|\,\G] \geq$ a.s.

\begin{exercise}
Prove that for all integrable $\xi$, we have $|\E[\xi|\G]| \leq \E[|\xi| \, | \G]$.
\end{exercise}

\begin{hint}
Note that $\xi = \xi^+ - \xi^-$ and utilise triangle inequality. Also note that $|\xi| = \xi^+ + \xi^-$.
\end{hint}

We can hence prove a conditional version of Jensen inequality:
\begin{theorem}[Conditional Jensen's Inequality]
Suppose that $(\Omega, \F,\p)$ is a probability space and $\xi$ is an integrable random variable taking values in an open interval $I \subset \R$. Let $g: I \to \R$ be convex and let $\G$ be a sub $\sigma-$algebra of $\F$. If $\E[|g(\xi)|] < \infty$, then
\begin{equation} \label{eq:conditional_Jensen_ineq}
    \E[\varphi(\xi) | \G] \ge \varphi(\E[\xi | \G])  \text{ almost surely.}
\end{equation}
\end{theorem}

\begin{property}[III. Tower/Telescopic Properties]
Let $(\Omega, \F,\p)$ be a probability space, $\xi$ an integrable random variable and $\F_1, \F_2$ be $\sigma-$algebras with $\F_1 \subseteq \F_2 \subseteq \F$. Then almost surely
\begin{equation}
\E[\E[\xi | \F_2] | \F_1] = \E[\xi | \F_1] = \E[\E[\xi | \F_1] | \F_2]
\end{equation}
\end{property}

\begin{hint}
This is a generalisation of exercise \ref{ex:tower_property}.
\end{hint}

\begin{proof}
The first equality can be proven by just using the definition: consider arbitrary $G \in \F_1$, then $G \in \F_2$ and hence 
\begin{equation}
    \E[\chi_G \E[\xi | \F_2]] = \E[\chi_G \xi] = \E[\chi_G \E[\xi | \F_1]]
\end{equation}
which establish the first equality. The second equality can be proven by using similar method in  question 3 from exercise \ref{ex:tower_property}, noting $\E[\xi\,|\,\F_1]$ is $\F_1$ measurable then it is also $\F_2$ measurable.
\end{proof}

\begin{property}[IV. On Taking Limits Under the Conditional Expectation Sign]
Let $\{ \xi_n \}_{n \ge 1}$ be a sequence of extended random variables.
\begin{enumerate}
    \item (Dominated convergence) If $|\xi_n| \le \eta$, $\E[\eta]<\infty$, and $\xi_n \to \xi$ $(a.s.)$, then
    \begin{equation*}
        \E[\xi_n \,|\, \G] \xrightarrow{a.s.} \E[\xi \,|\, \G]
    \end{equation*}
    and 
    \begin{equation*}
        \E[|\xi_n - \xi| | \G] \xrightarrow{a.s.} 0.
    \end{equation*}
    \item (Monotone convergence) If $\xi_n \ge \eta$, $\E[\eta] > -\infty$, $\xi_n \uparrow \xi \, (a.s.)$ and $\E[|\xi|] < \infty$, then
    \begin{equation*}
        \E[\xi_n | \G] \uparrow \E[\xi | \G] \quad (a.s.)
    \end{equation*}
    \item If $\xi_n \le \eta$, $\E[\eta] < \infty$, and $\xi_n \downarrow \xi \, (a.s.)$, then
    \begin{equation*}
        \E[\xi_n | \G] \downarrow \E[\xi | \G] \quad (a.s.)
    \end{equation*}
    \item (Fatou) If $\xi_n \ge \eta $, $\E[\eta] > - \infty$, then
    \begin{equation*}
        \E[\liminf \xi_n | \G] \le \liminf \E[\xi_n | \G] \quad (a.s.)
    \end{equation*}
    \item If $\xi_n \le \eta $, $\E[\eta] < \infty$, then
    \begin{equation*}
        \E[\limsup \xi_n | \G] \le \limsup \E[\xi_n | \G] \quad (a.s.)
    \end{equation*}
    \item (Summation) If $\xi_n \ge 0$ then
    \begin{equation*}
        \E\big[\sum \xi_n | \G \big] = \sum \E[\xi_n | \G] \quad (a.s.)
    \end{equation*}
\end{enumerate}
\end{property}

\begin{proof}
\begin{enumerate}
\item[]
\item Let $\zeta_n = \sup_{m\geq n} |\xi_m - \xi|$. Then $0 \leq |\zeta_n| \leq 2\eta$ and $\zeta_n \to 0$ almost surely, so by DCT we have $\E[\zeta_n] \overset{n\to\infty}\to 0$ Now by triangle inequality, one has 
\begin{equation*}
0 \leq \abs{\E[X_n|\G] - \E[X|\G]} \leq \E[|X_n-X| \,|\, \G] \leq \E[\zeta_n|\G].
\end{equation*}
Since the sequence $\E[\zeta_n|\G](\omega)$ is decreasing on $n$ for fixed $\omega$, the sequence $\E[\zeta_n|\G](\omega)$ exhibits limits $\omega$-almost surely. To evaluate the limit, notice that
\begin{equation*}
0 \leq \E\sqbracket{\lim_{n\to\infty} \E[\zeta_n|\G]} \leq \E\sqbracket{\E[\zeta_n|\G]} = \E[\zeta_n] \overset{n\to\infty}\to 0,
\end{equation*}
so $\lim_{n\to\infty} \E[\zeta_n|\G] = 0$ almost surely, completing the proof.
\item We note from the proof of MCT that one can set $\eta = 0$. Let $\tilde{\xi}_n = \E[\xi_n|\G]$. Then by property II (monotonicity) we have $\tilde{\xi}_n \geq 0$ almost surely and the events $A_n = \set{\tilde{\xi}_n < \tilde{\xi}_{n-1}} \in \G$ has $\p(A_n) = 0$. Let $\tilde{\xi} := \limsup_{n\to\infty} \tilde{\xi}_n$ and $A = \cup_{n\geq 2}$. Then $A \in \G$, $\p(A) = 0$ and for all $\omega \in A^c$ we know that $\tilde{\xi}_n(\omega) \nearrow \tilde{\xi}(\omega)$. We evaluate the limits by noting that for all $G \in \G$,
\begin{equation*}
    \E\sqbracket{\tilde{\xi} \chi_G} = \E\sqbracket{\tilde{\xi}\chi_{G \cap A^c}} \overset{\text{MCT}}= \lim_{n\to\infty} \E\sqbracket{\tilde{\xi}_n \chi_{G \cap A^c}} = \lim_{n\to\infty} \E\sqbracket{\xi \chi_{G \cap A^c}} \overset{\text{MCT}} = \E\sqbracket{\xi\chi_{G \cap A^c}} = \E\sqbracket{\xi\chi_{G}},
\end{equation*}
so we see that $\tilde{\xi}$ is integrable (by taking $G \in \G$) and that $\xi = \tilde{\xi}$ almost surely as desired.
\item This is trivially equivalent to (2) by considering the sequence $\xi - \xi_n$.
\item This is a direct application to (2) by considering the sequence $\eta_n = \inf_{k\geq n} \xi_n$, which is increasing.
\item This is trivially equivalent to (4).
\item This is a direct application to (2).
\end{enumerate}
\end{proof}

\begin{corollary}[V. Factorisation] \label{cor:conditional_expectation_factorisation}
Let $\xi$ and $\eta$ be random variables on $(\Omega, \F, \p)$ with $\xi$, $\eta$ and $\xi \eta $ integrable. Let $\G \subset \F$ be  a $\sigma$-algebra and suppose that $\eta$ is $\G-$measurable. Then
\begin{equation}
	\E[\xi \eta | \G] = \eta \E[\xi | \G] \text{ almost everywhere}
\end{equation}
\end{corollary}

\begin{hint}
Sorry this is not an easy proof. We utilise the four step proof, see chapter 1.
\end{hint}

We finally have the following
\begin{proposition}[$L^2$ orthogonal projection] \label{prop:conditional_expectation_proj}
Suppose $\xi$ is a random variable on $(\Omega,\F,\p)$ such that $\E[\xi^2] < \infty$ (i.e. $\xi \in L^2$). Then $\E[X\,|\,\G]$ is the $L^2$ orthogonal projection onto the subspace 
\begin{equation}
\mathcal{P} = \set{g \in L^2 \,|\, g \text{ is }\G\text{ measurable.}}
\end{equation}
In other words, $\E[X\,|\,\G]$ is the unique minimiser of $\E[X-Y]^2$ for $Y \in \G$.
\end{proposition}

\begin{proof}
We first note that $\E[\E[X\,|\,\G]^2] \leq \E[X^2]$ by conditional Jensen inequality \eqref{eq:conditional_Jensen_ineq}, and that $\E[X\,|\,\G]$ is $\G$-measurable, so $\E[X\,|\,\G] \in \G$. Let $Z$ be a $\G$-measurable function, then \begin{align*}
    \E[X-Z]^2 &= \E[X-\E[X\,|\,\G]+\E[X\,|\,\G]-Z]^2 \\
    &= \E[X-\E[X\,|\,\G]^2 + \E[X\,|\,\G]-Z]^2 + 2\E[(X-\E[X\,|\,\G])(\E[X\,|\,\G]-Z)]
\end{align*}
It remains to show that the cross term is zero. Notice that $\E[X\,|\,\G]-Z$ is $\G$-measurable, so by tower property (exercise \ref{ex:tower_property})
\begin{align*}
    \E[(X-\E[X\,|\,\G])(\E[X\,|\,\G]-Z)] &= \E[\E[(X-\E[X\,|\,\G])(\E[X\,|\,\G]-Z)\,|\, \G]] \\
    &= \E[(\E[X\,|\,\G]-\E[X\,|\,\G])(\E[X\,|\,\G]-Z)] = 0
\end{align*}
the last equality is justified by the factorisation property (corollary \ref{cor:conditional_expectation_factorisation}), linearity and another application of tower property. So we have
\begin{equation}
    \E[X-Z]^2 \geq \E[X-\E[X\,|\,\G]^2 + \E[X\,|\,\G]-Z]^2 \geq \E[X-\E[X\,|\,\G]^2
\end{equation}
with equality holds if $\E[X\,|\,\G]=Z$ almost everywhere.
\end{proof}

\subsection{Conditioning on a random variable}
We define the following
\begin{definition}
The \textit{conditional expectation} of a random variable $\xi$ with respect to a random variable $\eta$ is defined as follows
\begin{equation*}
    \E[\xi | \eta] \equiv \E[\xi | \sigma(\eta)],
\end{equation*}
where $\sigma(\eta)$ is the $\sigma-$algebra generated by $\eta$.
\end{definition}

\begin{theorem}[Representation of conditional expectation] \label{thm:rep_of_conditional_expectation}
There exists a unique (almost everywhere) Borel function $g: \R \to \R$ such that
\begin{equation}
    \E[\xi | \eta] = g(\eta).
\end{equation}
\end{theorem}

The proof is a direct application of the following lemma from measure theory

\begin{lemma}
Let $\mu, \eta$ be random variables on $(\Omega,\F,\p)$. Then $\mu$ be $\sigma(\eta)$-measurable $\iff$ there exists a Borel-measurable function $f: \R \to \R$ such that $\mu = f(\eta)$. 
\end{lemma}

\begin{hint}
We apply four-step proof on $\mu$.
\end{hint}

\begin{proof}
\begin{enumerate}
    \item[]
    \item Let $\mu = \chi_A$ for some $A \in \F$. For $\mu$ to be $\sigma(\eta)$ measurable we must have $A \in \sigma(\eta)$. (Why?) This means there exists $B \in \B(\R)$ such that $\eta^{-1}(B) = A$. We immediate see that $\mu = \chi_B(\eta)$.
    \item Let $\mu$ be a simple random variable such that $\mu = \sum_{i=1}^n c_j \chi_{A_j}$, with $\set{A_j}_{j=1}^n$ partitions $\Omega$. Again we must have $A_j \in \sigma(\eta)$, so for all $j$ there exists $B \in \B(\R)$ such that $\eta^{-1}(B_j) = A_j$. Then $\set{B_j}_{j=1}^n$ partitions $\eta(\Omega)$, and that $\mu = f(\eta)$ with $f = \sum_{i=1}^n c_j \chi_{B_j}(x)$.
    \item We assume $\mu$ being non-negative, then $\mu$ can be approximated by a pointwise non-decreasing sequence of simple random variable which converges pointwise to $\mu$: $\mu_n \nearrow \mu$. Each $\mu_n$ can be represented in the form of $f_n(\eta)$. Choose $f = \sup_{n\geq 1} f_n$, which is Borel measurable, and for all $\omega$, 
    \begin{equation}
        f(\eta(\omega)) = \sup_{n\geq 1} \mu_n(\omega) = \mu(\omega)
    \end{equation}
    \item For general $\mu$, we decompose it into $\mu^+ - \mu^-$, and write $\mu^+ = f^+(\eta)$ and $\mu^- = f^-(\eta)$ by step 3. We let $f = f^+ - f^-$ to complete the proof.
\end{enumerate}
\end{proof}

\begin{example}[Conditional expectation of random variables with joint density] \label{eg:conditional_expectation_joint_rv}
Consider real valued random variables $X,Y$ on same probability space $(\Omega,\F,\p)$. Assume the random vector $(X,Y)$ has continuous joint density $f_{X,Y}(x,y) > 0$. Recall that $X$ has density $f_X(x) = \int_\Omega f_{X,Y}(x,y) \, dy$ and $Y$ has density $f_Y(y) = \int_\Omega f_{X,Y}(x,y) \, dx$. Assume $f_X(x), f_Y(y) > 0$ almost everywhere in $\R$. We want to compute $\E[h(X)\,|\,Y]$ for all Borel-measurable function $h$ with $\E[|h(X)|] < \infty$. By theorem \ref{thm:rep_of_conditional_expectation}, we know that $\E[h(X)\,|\,Y] = \phi(Y)$ for a unique (almost everywhere) Borel-measurable $\phi$. We claim that
\begin{equation} \label{eq:conditional_expectation_rv}
    \phi: y \mapsto \int_\R h(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx
\end{equation}
We can show this by utilising the definition of conditional expectation: for all $A \in \sigma(Y)$ we have
\begin{equation}
    \E[\chi_A \phi(Y)] = \E[\chi_A h(X)]
\end{equation}
Since $A \in \sigma(Y) \subseteq \F$, we know that $A = Y^{-1}(B)$ for some $B \in \B(\R)$.

\begin{align*}
    \E[\chi_A h(X)] &= \E[\chi_B(Y) h(X)] \\
    &= \int_\R \int_\R h(x) \chi_B(y) f_{X,Y}(x,y) \, dy \, dx \\
    &= \int_\R \int_\R h(x) \chi_B(y) \frac{f_{X,Y}(x,y)}{f_Y(y)} f_Y(y) \, dy \, dx \\
    &\overset{\text{(Tonelli)}}{=} \int_\R \int_\R h(x) \chi_B(y) \frac{f_{X,Y}(x,y)}{f_Y(y)} f_Y(y) \, dx \, dy \\
    &= \int_\R \chi_B(y) \bracket{\int_\R h(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx} \, f_Y(y) dy \\
    &= \E[\chi_B(Y) \phi(Y)] \\
    &= \E[\chi_A \phi(Y)]
\end{align*}
which completes the proof. In particular, this shows us that if $C = X^{-1}(D)$
\begin{equation} \label{eq:regular_conditional_distribution_joint_rv}
    \p(X \in C \,|\, Y) = Q(Y;C), \; \text{with} \; Q(y;C) = \int_\R \chi_C(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx
\end{equation}
This example can be generalised to the case when $X,Y$ are random vectors.
\end{example}

\begin{exercise}[Conditional expectation of discrete random variables] \label{ex:conditional_expectation_discrete_rv}
\begin{itemize}
    \item[]
    \item Consider random variables $X,Y$ taking value in $\N$, and assume they have joint mass $p_{X,Y}(x,y)$ for $x,y \in \N$. Assume $h:\N \to \R$ such that $\E[|h(X)|] < \infty$. Using \eqref{eq:conditional_probability_partition}, verify that $\E[h(X)\,|\,Y] = \phi(Y)$, where for $y$ such that $p_Y(y) \neq 0$,
    \begin{equation}
        \phi(y) = \sum_{x\in\N} h(x) \frac{p_X(x,y)}{p_Y(y)}
    \end{equation}
    Notice how this formula is similar to \eqref{eq:conditional_expectation_rv}. What value should we assign to $\phi(y)$ for $p_Y(y) = 0$ if we were to follow our convention about conditional probability on zero-probability events?
    \item Here is an application to the above formula. Consider random variables $Z_1, Z_2$ on $(\Omega,\F,\p)$ with $Z_1 \sim \Po(\lambda_1)$ and $Z_2 \sim \Po(\lambda_2)$. Assume $p = \lambda_1/(\lambda_1+\lambda_2)$, show that
    \begin{equation}
        \p[Z_1 = k \,|\, Z_1 + Z_2 = n] = {n \choose k} p^k (1-p)^{n-k}
    \end{equation}
\end{itemize}
\end{exercise}

We will study the above examples further in the next section. Before that, let us highlight the projection property of conditional expectation.

\begin{proposition}[$L^2$ orthogonal projection II]
If $\E[\xi^2] < \infty$, then 
\begin{equation*}
    \min_f \E[(\xi - f(\eta)^2] = \E[(\xi - \E[\xi | \eta])^2],
\end{equation*}
where min is over all $\sigma(\eta)$-measurable functions such that $\E[f^2(\eta)] < \infty$.
\end{proposition}

\begin{proof}
This is a direct application of proposition \ref{prop:conditional_expectation_proj}.
\end{proof}

We can therefore obtain conditional expectation by obtaining the minimiser of $L^2$ error.

\begin{exercise}[Conditional expectation of normal distribution] Consider random variables $X,Y$ such that they are jointly normally distributed:
\begin{equation}
    \begin{pmatrix} X \\ Y\end{pmatrix} \sim \mathbf{N}_{2} \bracket{ \begin{pmatrix} \mu_X \\ \mu_Y \end{pmatrix}, \begin{pmatrix} \sigma_X^2 & \rho \sigma_X \sigma_Y \\ \rho \sigma_X \sigma_Y & \sigma_Y^2 \end{pmatrix}},
\end{equation}

then $\E[Y|X] = f(X)$. Assume we know that $f(x) = ax+b$, then we know that $a,b$ are minimiser of $\E[Y - aX - b]^2$. Verify that
\begin{equation}
    \E(Y\,|\,X) = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X}(X - \mu_X) = \bracket{\mu_Y - \rho \frac{\sigma_Y}{\sigma_X}\mu_X} + \rho \frac{\sigma_Y}{\sigma_X} X.
\end{equation}
\end{exercise}
\
\subsection{Regular conditional distribution}
\begin{unexaminable}
Let us revisit the above examples. We know from example \ref{eg:conditional_expectation_joint_rv} that if $(X,Y)$ has joint density $f_{X,Y}(x,y) > 0$ then we have
\begin{equation}
    \p(X \in C \,|\, Y) = Q(Y;C), \; \text{with} \; Q(y;C) = \int_\R \chi_C(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx.
\end{equation}

There are two ways to interpret $Q(y;C)$. If we fix a set $C \in \F$, then the function $Q(.;C)$ is a $\sigma(Y)$-measurable. On the other hand, if we fix $y \in Y(\Omega)$, then the set function $Q(y;.)$ is a measure on $(\R,\B(\R))$. To see this it is trivial to see that $Q(y;\varnothing) = 0$ and 
\begin{equation}
    Q(y,\R) = \frac{1}{f_Y(y)} \int_\R f_{X,Y}(x,y) \, dx = 1 
\end{equation}

We only need to prove $\sigma$-additivity. Let $A_1, A_2, ...$ be disjoint sets in $\B(\R)$, and let $A = \sqcup_{i\geq 1} A_i$, then by monotone convergence theorem
\begin{align*}
    Q(y,A) = \int_\R \chi_A(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx 
    &= \int_\R \sum_{i\geq 1} \chi_{A_i}(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx \\ &\overset{\text{(MCT)}}{=} \sum_{i\geq1} \int_\R \chi_{A_i}(x) \frac{f_{X,Y}(x,y)}{f_Y(y)} \, dx = \sum_{i\geq 1} Q(y,A_i)
\end{align*}

$Q(y,C)$ is hence considered as a \textit{(Markov) stochastic kernel}. In this chapter, we would like to prove that we can construct such transitional kernel $Q_{Y,\G}(\omega, C) = [\p(Y \in C \,|\, \G)](\omega)$ for any random variables $Y$ and $\sigma$-algebra $\G$. We begin by formally defining the notion of transitional kernel

\begin{definition}[Transitional, Stochastic kernel]
Let $(\Omega_1, \F_1)$, $(\Omega_2,\F_2)$ be measurable spaces. A map $Q:\Omega_1 \times \F_2 \to [0,\infty]$ is a \textit{transitional kernel} if
\begin{itemize}
    \item when $A_2 \in \F_2$ is fixed, $Q(.,A_2)$ is a $\F_1$-measurable function.
    \item when $\omega_1 \in \Omega_1$ is fixed, the set function $Q(\omega_1,.)$ is a measure on $(\Omega_2, \F_2)$
\end{itemize}
In addition if $\forall \omega_1 \in \Omega_1$, $Q(\omega_1, \Omega_2) = 1$ (i.e. $Q(\omega_1,.)$ is a probability measure), then $Q$ is a \textit{(Markov) stochastic kernel}. If we have $\forall \omega_1 \in \Omega_1$, $Q(\omega_1, \Omega_2) \leq 1$, then $Q$ is \textit{substochastic}.
\end{definition}

\begin{remark}
A time-homogeneous Markov chain is characterise by a family of stochastic kernels, which explains why stochastic kernels are named after Markov.
\end{remark}

From this, we can define the notion of regular conditional distribution. Here we assume $\xi$ takes value on a general measurable space $(E,\mathcal{E})$.

\begin{definition}[Regular conditional distribution]
Let $\xi:(\Omega,\F) \to (E,\mathcal{E})$ be a random variable, and let $\G$ be a $\sigma$-algebra of $\Omega$ contained in $\F$. A regular conditional distribution of $\xi$ \textbf{given the $\sigma$-algebra} $\G$ is a stochastic kernel $Q: \Omega \times \mathcal{E} \to [0,\infty]$ such that for almost all $\omega \in \Omega$, we have 
\begin{equation} \label{eq:rcd}
    \forall B \in \mathcal{E}, \quad Q(\omega, B) = [\p(\xi \in B \,|\, \G)](\omega)
\end{equation}
In addition, assume $\eta: (\Omega, \sigma(\eta)) \to (E',\mathcal{E}')$ is a random variable (not necessary equal to $(E,\mathcal{E})$), then the conditional distribution of $\xi$ given the \textbf{random variable $\eta$} is the conditional distribution of $\xi$ given on the $\sigma$-algebra $\sigma(\eta)$.
\end{definition}

\begin{remark} \label{rmk:rcd}
Let $Q:\Omega \times \E$ be a regular conditional distribution of $\xi$ given $\eta$, then for almost all $\omega \in \Omega$, and for all $B \in \mathcal{E}$, we have
\begin{equation}
    Q(\omega,B) = [\p(\xi \in B \,|\, \eta)](\omega)
\end{equation}
When $Q$ is restricted so that the above equality is satisfied, then $Q(.,B)$ is $\eta$ measurable, and hence $Q(\omega,B) = \tilde{Q}(\eta(\omega), B)$ for a unique function $\tilde{Q}: (E',\mathcal{E}') \times \mathcal{E} \to [0,\infty]$, such that $\tilde{Q}(.,B)$ is $\mathcal{E}$ measurable for all $B \in \mathcal{E}$. We will also refer to this function $\tilde{Q}$ when talking about regular conditional distribution of $\xi$ on $\eta$.
\end{remark}

\begin{example}[Continuation of Example \ref{eg:conditional_expectation_joint_rv}] Under same settings, we see that $Q(y,C)$ is a regular conditional distribution of $X$ given $Y$ in the sense as described in remark \ref{rmk:rcd}. Note that when $y$ is fixed, then the measure $Q(y,.)$ is absolutely continuous with density $f_{X|Y}(x|y) := f_{X,Y}(x,y)/f_Y(y)$. The function $f_{X|Y}(x|y)$ is known as the \textit{conditional density} of $X$ given $Y$.
\end{example}

\subsubsection{Existence of regular conditional distribution}

We first assume $(E,\mathcal{E})$ to be $(\R,\B(\R))$ for simplicity. The main difficulty of constructing appropriate regular conditional distribution is that the conditional expectation $[\p(Y \in B \,|\, \G)](\omega)$ is unique up to measure-zero set. Moreover, many properties of probability measures only hold almost surely, e.g. monotonicity and continuity from above/below. Fortunately, we only need the equality \eqref{eq:rcd} to hold for almost all $\omega$. Our plan is therefore to construct the stochastic kernel $Q(\omega,B)$ for almost all "good" $\omega$ when all desired properties hold, and extend this kernel to other "bad" $\omega$. This will work if the set of "bad" $\omega$ is measure zero.

\begin{theorem}[Existence of regular conditional distribution for real-valued random variables] 
Let $\xi:(\Omega,\F,\p) \to (\R,\B(\R))$ be a random variable, then for all $\sigma$-algebra $\G \subseteq \F$ we can construct a regular conditional distribution $\xi$ given $\G$.
\end{theorem}

\begin{hint}
We only need to look at $Q(\omega,B)$ for all $B = (-\infty,r]$ for some $r$, since the half intervals $(-\infty,r]$ generates $\B(\R)$. From chapter 1, we list out the desired property for a version of $F(\omega,r) = [\p[\xi \in (-\infty,r] \,|\, \G]](\omega)$ to be a valid distribution function
\begin{enumerate}
    \item $F(\omega,r) \overset{r \to -\infty}{\to} 0$ and $F(\omega,r) \overset{r \to \infty}{\to} 1$
    \item (Monotonicity) $F(\omega,r) \leq F(\omega,s)$ whenever $r \leq s$, and
    \item (Right continuity) $F(\omega, (-\infty,r_n)) \searrow F(\omega,r)$ whenever $r_n \searrow r$.
\end{enumerate}
So there are four cases for $F(\omega, r)$ not to be a distribution function:
\begin{itemize}
    \item $\omega \in A_{r,s} := \set{\omega \,|\, F(\omega,r) > F(\omega,s)}$ for $r \leq s$.
    \item $\omega \in B_r' := \set{\omega \,|\, \text{there exists a sequence } (r_n) \searrow r \text{ such that } F(\omega,r_n) \not\rightarrow F(\omega,r)}$
    \item $\omega \in C' := \set{\omega \,|\, F(\omega,r) \overset{r\to \infty}{\not\rightarrow} 1}$
    \item $\omega \in D' := \set{\omega \,|\, F(\omega,r) \overset{r\to -\infty}{\not\rightarrow} 0}$
\end{itemize}
If we can show that the "bad" set of $\omega$, $G' = (\cup_{r\leq s} \, A_{r,s}) \cup (\cup_r \, B_r) \cup C \cup D$, has measure zero, then we can define $F(\omega,.)$ as followed:
\begin{equation}
    F(\omega, r) = \begin{cases}
    [\p(\xi \in (-\infty, r] \,|\, \G)](\omega) & \omega \in \Omega \setminus G' \\
    F_0(r) & \omega \in F
    \end{cases}
\end{equation}
where $F_0$ can be any appropriate distribution function. Then for all $\omega$, $F(\omega,r)$ is a distribution function, and for almost all $\omega$, $F(\omega,r) = \p(\xi \in (-\infty,r] \,|\, \G)$. \\

We note from property \ref{prop:conditional_expectation_monotonicity} that $A_{r,s}$ has measure zero for any fixed $r,s$, but this does not imply that $\cup_{r\leq s} A_{r,s}$ has measure zero if we only assume $r,s$ are real, since this is an uncountable union. Moreover, even though we know that for a fixed sequence $(r_n) \searrow r$, we have $F(\omega,r_n) \searrow F(\omega,r)$ almost surely in $\omega$, we don't know whether $B_r$ itself has measure zero, since
\begin{equation}
    B_r' = \bigcup_{(r_n) \text{ sequence} \\ (r_n) \searrow r} \set{\omega \,|\, F(\omega,r_n) \not\to F(\omega,r)}
\end{equation}
which is again an uncountable union of measure zero sets. Fortunately, we can refine our analysis by noting the following elementary facts in analysis: (exercise!)
\begin{itemize}
    \item If $f:\R \to \R$ is a monotonic increasing function, then $f$ is right continuous iff for all $x$, there exists a sequence $(x_n) \searrow x$ such that $f(x_n) \to x$ as $n \to \infty$.
    \item $\Q$ is dense in $\R$. As a result, the half-intervals $(-\infty,a], a\in \Q$ generate $\B(\R)$.
\end{itemize}
As a result, we may refine our analysis by considering $F(\omega,r)$ for all $r \in \Q$, then extend our analysis to $r \in \R$.
\end{hint}

\begin{proof}
We redefine the sets of "bad" $\omega$.
\begin{itemize}
    \item $\omega \in A_{r,s} := \set{\omega \,|\, F(\omega,r) > F(\omega,s)}$ for $r \leq s$
    \item $\omega \in B_r := \set{\omega \,|\, F(\omega, r + 1/n) \not\rightarrow F(\omega,r)}$
    \item $\omega \in C := \set{\omega \,|\, F(\omega,n) \overset{n\to \infty}{\not\rightarrow} 1}, n \in \Z_{\geq 1}$
    \item $\omega \in D := \set{\omega \,|\, F(\omega,-n) \overset{n\to \infty}{\not\rightarrow} 0}, n \in \Z_{\geq 1}$.
\end{itemize}
Define 
\begin{equation}
    G = \bracket{\bigcup_{r\leq s, \\ r,s \in \Q} A_{r,s}} \cup \bracket{\bigcup_{r \in \Q} B_r} \cup C \cup D
\end{equation}
Then $G$ is a countable union of measure-zero sets, and so $G$ is also measure-zero. Hence if we define
\begin{equation}
    F(\omega, z) = \begin{cases}
    \inf\bracket{[\p(\xi \in (-\infty, r] \,|\, \G)](\omega) \,:\, r \in \Q, r > z} & \omega \in \Omega \setminus G \\
    F_0(z) & \omega \in F
    \end{cases}
\end{equation}
then it is a valid distribution function for all $\omega$, and by chapter 1 we can assign a measure $Q(\omega,.)$ to each of the distribution $F(\omega,.)$. Be reminded that $Q(\omega,B)$ is itself a measurable function for all $B \in \B(\R)$, we have therefore constructed a stochastic kernel. \\

We finally check that $Q$ is actually a regular conditional distribution, i.e. for all $A \in \G$,
\begin{equation}
    \E[\chi_A(\omega) Q(\omega, B)] = \E[\chi_{A \cap \set{\xi \in B}}] = \p(A \cap \set{\xi \in B})
\end{equation}
Fix an arbitrary $A \in \G$, then we see that the above equality holds for all $B = (-\infty,r]$ where $r \in \Q$, and hence we can extend the equality to for all $B \in \B(\R)$.
\end{proof}

\begin{remark}
We can extend the above result to any Polish spaces $(E,\mathcal{E})$, for instance $\R^n$ and $C^0([0,1])$ equipped with supremum norm. To do so (theoretically) we note that there is a bijective map $\varphi:(E,\mathcal{E}) \to (\R,\B(\R))$ such that $\varphi$ is $\mathcal{E}$ measurable and $\varphi^{-1}$ is $\B(\R)$ measurable. We can hence construct regular conditional distribution on $\xi:(\Omega,\F) \to (E,\mathcal{E})$ by constructing a regular conditional distribution on $\xi' = \phi \circ \xi: (\Omega,\F) \to (\R,\B(\R))$, denote as $\overline{Q}:\Omega \times \B(\R) \to [0,\infty]$. Then the stochastic kernel $Q(\omega,A) = \overline{Q}(\omega,\phi(A)) : \Omega \times \mathcal{E} \to [0,\infty]$ is indeed a regular conditional distribution of $\xi$.
\end{remark}

\subsubsection{Further Examples}

\begin{example}[Continuation of exercise \ref{ex:conditional_expectation_discrete_rv}]
 Consider random variables $Z_1, Z_2$ on $(\Omega,\F,\p)$ with $Z_1 \sim \Po(\lambda_1)$ and $Z_2 \sim \Po(\lambda_2)$. Assume $p = \lambda_1/(\lambda_1+\lambda_2)$. We have shown that the following regular conditional distribution of $Z_1$ given $Z_1 + Z_2$
\begin{equation}
    \p[Z_1 = k \,|\, Z_1 + Z_2 = n] = {n \choose k} p^k (1-p)^{n-k}
\end{equation}
is actually the probability mass of a binomial distribution $\mathsf{B}(n,p)$. As a result, we may abuse definitions to say that of $Z_1$ given $Z_1 + Z_2$ is "$\mathsf{B}(n,p)$" distributed, written as $Z_1 \,|\, Z_1 + Z_2 \sim \mathsf{B}(n,p)$.
\end{example}

\begin{exercise}[Evaluating conditional expectation from conditional distribution]
Let $Q(\omega,B)$ be a regular conditional distribution of random variable $\xi: (\Omega,\F,\p) \to (E,\mathcal{E})$ given $\sigma$-algebra $\G\subseteq\F$. Assume $h:(E,\mathcal{E}) \to (\R,\B(\R))$ is a $\mathcal{E}$-measurable function such that $\E|h(\xi)| < \infty$. 
\begin{enumerate}
    \item Show that
\begin{equation}
    [\E[h(\xi)\,|\,\G]](\omega) = \int h(x) Q(\omega, dx) 
\end{equation}
where the integral at the RHS is defined so that
\begin{equation}
    \int \chi_B(x) Q(\omega, dx) = \int \chi_B(x) \, dQ(\omega, .) =  Q(\omega,B)
\end{equation}
    \item Define the map $\overline{Q}$ mapping $f$ as a bounded function on $(E,\mathcal{E})$ to a function $\overline{Q}f$ on $(\Omega,\F)$ such that:
    \begin{equation}
        \overline{Q}: f \mapsto \bracket{\omega \mapsto \int f(x) \, Q(\omega, dx)}
    \end{equation}
    Show that $\overline{Q}f$ is bounded, and that $\overline{Q}$ is a contraction int the following sense:
    \begin{equation}
        \sup_{\omega \in \Omega} |\overline{Q}f(\omega)| \leq \sup_{x \in E} |f(x)|
    \end{equation}
\end{enumerate}
\end{exercise}

\begin{hint}
For question 1, one can prove the desired result to $h$ being simple function by using linearity of integrals. Then follow the remaining steps of the four-step proof by utilising appropriate convergence theorems.
\end{hint}

\begin{exercise}[Bayesian Analysis]
\begin{enumerate}
    \item[]
    \item Construct a random vector $(\lambda,N): (\Omega,\F) \to ([0,\infty), \B([0,\infty))) \otimes (\N, 2^\N)$ such that $\lambda \sim \Gamma(\alpha,\beta)$ and $N \,|\, \lambda \sim \Po(\lambda)$.
    \item Show that $\lambda \,|\, N$ follows a Poisson distribution with a suitable parameter as a function of $N$.
\end{enumerate}
\end{exercise}
\end{unexaminable}